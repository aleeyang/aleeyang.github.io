<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Yang Fu</title>
    <meta name="description" content="Yang Fu is a PhD student at Fudan University.">
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
    <link
        href="https://fonts.googleapis.com/css2?family=Helvetica+Neue:wght@300;400;700&family=Oswald:wght@700&display=swap"
        rel="stylesheet">
</head>

<body>

    <nav class="navbar">
        <div class="container">
            <div class="nav-links">
                <a href="index.html">Home</a>
                <!-- <a href="#">Personal</a> -->
            </div>
        </div>
    </nav>

    <div class="container main-container">
        <!-- Sidebar (col-md-4 equivalent) -->
        <div class="sidebar">
            <img src="./assets/img/avatar.jpg" alt="Yang Fu" class="profile-img">
            <div class="profile-info">
                <div class="profile-name">Yang Fu</div>
                <div class="profile-email">aleeyanger [at] gmail.com</div>

                <div class="social-links">
                    <a href="https://github.com/aleeyang" title="GitHub"><i class="fab fa-github"></i></a>
                    <a href="https://x.com/haha19991372305" title="Twitter"><i class="fab fa-twitter"></i></a>
                    <a href="https://scholar.google.com/citations?user=Greu79UAAAAJ&hl" title="Google Scholar"><i
                            class="fas fa-graduation-cap"></i></a>
                </div>
            </div>
        </div>

        <!-- Main Content (col-md-8 equivalent) -->
        <div class="main-content">

            <section id="about">
                <h2>About Me</h2>
                <p>
                    I am a PhD student at <a href="https://www.fudan.edu.cn/en/">Fudan University</a> and researcher
                    intern at TikTok. During my master’s studies, I was fortunate to be mentored by
                    <a href="https://scholar.google.com/citations?user=OH7-k7oAAAAJ&hl=zh-CN" target="_blank"
                        rel="noopener noreferrer">
                        Prof.Yongzhen Huang
                    </a>
                    and
                    <a href="https://www.a-star.edu.sg/cfar/about-cfar/our-team/dr-hwee-kuan-lee" target="_blank">
                        Hwee Kuan</a>,
                    who not only guided my academic growth but also shaped my perspectives and aspirations.
                </p>
                <p>
                    Previously, I obtained my M.Eng. from Beijing Normal University, advised by <a
                        href="https://scholar.google.com/citations?user=OH7-k7oAAAAJ&hl=en">Prof. Yongzhen Huang</a>,
                    and my B.Eng. from Xihua University. I have interned at A*STAR (2025), National University of
                    Singapore (2025), MIT-IBM Watson AI Lab (2024), and Watrix AI (2022, 2023).
                </p>
            </section>

            <section id="interests">
                <h2>Research Interests</h2>
                <p>
                    My research focuses on the creation, editing, and understanding of videos and visual media,
                    aiming to build human–AI collaborative intelligence that enhances creativity and productivity.
                </p>
                <p>
                    <b>Learning Methods</b> generative models, reinforcement learning, human motion analysis
                </p>
                <p>
                    <b>Software Systems</b> blender, unreal engine, unity
                </p>

            </section>

            <section id="news">
                <h2>News</h2>
                <ul class="news-list">
                    <li>One paper accepted to NeurIPS 2025.</li>
                    <li>Interned at TikTok.</li>
                    <li>One paper accepted to MM 2025.</li>
                    <li>One paper accepted to TPAMI 2025.</li>
                    <li>Interned at A*STAR.</li>
                    <li>Invited talk at Gait 2024 @ SUSTech.</li>
                    <li>Won 1st Place (Multimodal) & 3rd Place (Pose) at ACM MM’24 Gait Challenge.</li>
                    <li>One paper accepted to ECCV 2024.</li>
                    <li>Open-source gait recognition framework released on arXiv.</li>
                    <li>One paper accepted to ICCV 2023.</li>
                </ul>
            </section>

            <section id="publications">
                <h2>Publications</h2>

                <p>
                    <a href="#">TalkCuts: A Large-Scale Dataset for Multi-Shot Human Speech Video Generation</a>
                    <br>
                    Jiaben Chen, Zixin Wang, Ailing Zeng, <b>Yang Fu</b>, Xueyang Yu, Siyuan Cen, Julian Tanke, Yihang
                    Chen, Koichi Saito, Yuki Mitsufuji, Chuang Gan.
                    <br>
                    NeurIPS 2025.
                </p>

                <p>
                    <a href="https://ieeexplore.ieee.org/document/11029177/">Bridging the Past and Future: In Defense of
                        Pose-based Gait Recognition</a>
                    <br>
                    Shibei Meng*, <b>Yang Fu*</b>, Saihui Hou, Xuecai Hu, Yongzhen Huang.
                    <br>
                    TPAMI 2025.
                </p>

                <p>
                    <a href="#">Seeing from Magic Mirror: Contrastive Learning from Reconstruction for Pose-based Gait
                        Recognition</a>
                    <br>
                    Shibe Meng, Saihui Hou, <b>Yang Fu</b>, Xuecai Hu, Chunshui Cao, Xu Liu, Yongzhen Huang.
                    <br>
                    MM 2025.
                </p>

                <p>
                    <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04501.pdf">Cut out the Middleman:
                        Revisiting Pose-based Gait Recognition</a>
                    <br>
                    <b>Yang Fu</b>, Saihui Hou, Shibe Meng, Xuecai Hu, Chunshui Cao, Xu Liu, Yongzhen Huang.
                    <br>
                    ECCV 2024.
                </p>

                <p>
                    <a
                        href="https://openaccess.thecvf.com/content/ICCV2023/papers/Fu_GPGait_Generalized_Pose-based_Gait_Recognition_ICCV_2023_paper.pdf">GPGait:
                        Generalized Pose-based Gait Recognition</a>
                    <br>
                    <b>Yang Fu*</b>, Shibe Meng*, Saihui Hou, Xuecai Hu, Yongzhen Huang.
                    <br>
                    The Thirty Fourth IEEE/CVF Conference on International Conference on Computer Vision (ICCV), 2023.
                </p>

                <p>
                    <a href="https://ieeexplore.ieee.org/document/10243069/">Gait Recognition with Drones: A
                        Benchmark</a>
                    <br>
                    Aoqi Li, Saihui Hou, Qingyuan Cai, <b>Yang Fu</b>, Yongzhen Huang.
                    <br>
                    TMM 2023.
                </p>

                <p>
                    <a href="https://arxiv.org/abs/2309.00794">FastPoseGait: A Toolbox and Benchmark for Efficient
                        Pose-based Gait Recognition</a>
                    <br>
                    Shibei Meng*, <b>Yang Fu*</b>, Saihui Hou, Xuecai Hu, Yongzhen Huang.
                    <br>
                    Technical Report, 2023.
                </p>

            </section>

            <section id="teaching">
                <h2>Teaching</h2>
                <h4>I served as a teaching / lab assistant for the following courses.</h4>
                <p>
                    BNU Intelligent Perception and Mobile Computing (Spring 2023)
                </p>
                <p>
                    BNU Deep Learning (Fall 2023)
                </p>
                <p>
                    BNU Deep Learning (Fall 2024)
                </p>
            </section>

            <section id="miscellaneous">
                <h2>Miscellaneous</h2>
                <p>
                    <b>Languages</b> Chinese, English
                </p>
                <p>
                    <b>Programming</b> Python, C/C++, Full-Stack, SQL
                </p>

            </section>

            <footer>
                <p class="copyright">
                    Copyright &copy;
                    <script>document.write(new Date().getFullYear())</script> Made by Yang Fu
                    <br>
                    <a href="https://github.com/mavroudisv/plain-academic">Template from Plain Academic</a>
                </p>
            </footer>

        </div>
    </div>

</body>

</html>